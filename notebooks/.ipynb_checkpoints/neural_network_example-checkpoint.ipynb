{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Example\n",
    "\n",
    "This notebook provides a brief (but useful!) intro to using neural networks in Python.\n",
    "\n",
    "For this, we will use the `sklearn` package and use it to classify our breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "# Size, passed to `plt.figure(figsize=FIGSIZE)`\n",
    "FIGSIZE = (10, 6)\n",
    "\n",
    "# DPI for saving the figure\n",
    "FIGDPI = 200\n",
    "\n",
    "# Number of points to use for toy distributions\n",
    "NPTS = 200\n",
    "\n",
    "# Pull matlab colormap\n",
    "cmap = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# Random Seed\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "As always, we load the data into Python using the `pandas` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up path to the sample data\n",
    "DATAPATH = os.path.join('data.csv')\n",
    "df = pd.read_csv(DATAPATH)\n",
    "\n",
    "# Display sample of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "First, we pull out the relevant features -- for this network, we are interested in the texture mean and radius mean of the dataset. \n",
    "We pull these out into `numpy` arrays, and then stack them so that we have an $N \\times d$ matrix, where $N$ is the number of samples and $d=2$ is our dimensionality (i.e. the number of \"input\" units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# Use this code to classify using only two features\n",
    "# =================================================\n",
    "\n",
    "X1 = np.array(df.texture_mean)\n",
    "X2 = np.array(df.radius_mean)\n",
    "X = np.transpose(np.vstack((X1, X2)))\n",
    "\n",
    "# =================================================\n",
    "# Use this code to classify using all features\n",
    "# =================================================\n",
    "# X = np.array(df.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to grab a label vector of $N\\times1$ size, indicating the labels for each sample. `sklearn` provides a nice function for this, `preprocessing.LabelEncoder()`, which you can use to transform the `pandas` dataframe into something usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text labels into numbers\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['B', 'M'])\n",
    "y = le.transform(df.diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data matrix $\\mathbf{X}$ and our label vector $\\mathbf{y}$, we have to separate them into training and testing sets. Again, `sklearn` provides a convenience function for this, `train_test_split()`, which allows you to specify a percentage of data to use for testing. Here we set `test_size=.3`, meaning that we are using 30% of the data for testing and 70% of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to scale the data. This helps the neural network in training by providing a nice set of inputs to learn from, and can help with convergence. The `StandardScalar()` object calculates the scaled feature $i$ vector as:\n",
    "\n",
    "$$ \\mathbf{x}_{i}^{\\prime} = \\frac{\\mathbf{x}_{i} - \\mu_{i}}{\\sigma_{i}} $$\n",
    "\n",
    "Where $\\mathbf{x}_{i}$ is the $i$-th feature, and $\\mu_{i}$ and $\\sigma_{i}$ are the mean and standard deviation for that feature. The result is a vector which has $\\mu = 0$ and $\\sigma = 1$. Note that each feature is scaled independently from the others. \n",
    "\n",
    "Also, note that we need to calculate $\\mu$ and $\\sigma$ on the **TRAINING** data, and then apply that scaling to both the training and the testing data. This is because we aren't supposed to know about the testing data yet -- thus, the mean and standard deviation shouldn't include these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "At last, we train! First, we create the classification \"object\" for a multilayer perceptron. You can look [here at the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) for a full list of the input parameters that you can specify.\n",
    "\n",
    "Here, we specify the following:\n",
    "\n",
    "- `solver`: \"sgd\" for Stochastic Gradient Descent\n",
    "- `alpha`: This is the L2-Regularization term -- basically it helps smooth out the gradients during training\n",
    "- `max_iter`: Tells the network to stop training at a certain number of iterations (otherwise it would keep going until the network \"converges\", which may take a really long time)\n",
    "- `hidden_layer_sizes`: Specifies the \"structure\" of the network -- how many hidden layers, and how many neurons per layer?\n",
    "- `random_state`: Allows us to reproduce the training if we run the algorithm again -- remember, the process is not deterministic, so you may get different results if you run multiple times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try hidden_layer_sizes = (4), (4, 4), (4, 4, 2)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, max_iter=2000,\n",
    "                    hidden_layer_sizes=(4), random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our classifier object, we can simply fit the model to the data (another way of saying \"training\"). This command will also print out a summary of all the options of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the performace of the fit classifier is on the training data. This gives us an idea if we're doing well at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_train, y_train, cv=3)\n",
    "print('Accuracy: {:0.2f}% (+/- {:0.2f})'.format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Network\n",
    "\n",
    "Now all that's left to do is apply the trained classifier to our source data. Most classifiers in `sklearn` have a `predict` function, but in this case we are also interested in the probabilities of the testing data, so let's use `predict_proba()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the fitted classifier to our data (overfitting!)\n",
    "x_predict = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How have we done? Let's look at the probabilities for some of the data (first ten elements), along with their ground truth labels. We'll also print out the predicted class, which is just which label had the higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P1\\tP2\\tPc\\tLabel\\n')\n",
    "for i in range(10):\n",
    "    print('{:0.2f}\\t{:0.2f}\\t{:d}\\t{:d}\\n'.format(x_predict[i,0], x_predict[i,1], np.argmax(x_predict[i,:]), y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Let's get a performance evaluation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, x_predict[:,1])\n",
    "auc = roc_auc_score(y_test, x_predict[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "f, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(fpr, tpr, color=cmap[0], label=\"ROC Curve\")\n",
    "plt.plot(np.linspace(0,1), np.linspace(0,1), color=cmap[1], label=\"0.5\")\n",
    "\n",
    "# Tweak the plot\n",
    "ax.legend(frameon=True, loc='upper left')\n",
    "ax.set(xlabel='False Positive Rate',\n",
    "      ylabel='True Positive Rate',\n",
    "      title='ROC Curve (AUC: {:.3f})'.format(auc))\n",
    "ax.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "It's always a good idea to visualize what you've done to verify that your classifier did what you expected it to do. So here, we'll plot out a region of space defined by the feature limits, then apply the classifier and look to see where our samples land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Contours\n",
    "# step size in the mesh\n",
    "h = .02  \n",
    "\n",
    "cm = plt.cm.RdBu_r\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "if hasattr(clf, \"decision_function\"):\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "else:\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "f, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    \n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "# Plot also the training points\n",
    "# ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "#            edgecolors='black', s=80, alpha=.8)\n",
    "ax.scatter(X_train[y_train==0,0], X_train[y_train==0,1], edgecolors='black', s=80, alpha=.8, label='Benign')\n",
    "ax.scatter(X_train[y_train==1,0], X_train[y_train==1,1], edgecolors='black', s=80, alpha=.8, label='Malignant')\n",
    "\n",
    "# ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "#            edgecolors='black', s=35)\n",
    "\n",
    "ax.legend(frameon=True, loc='upper left')\n",
    "ax.set(xlabel='Texture Mean',\n",
    "       ylabel='Radius Mean',\n",
    "       xlim=(xx.min(), xx.max()),\n",
    "       ylim=(yy.min(), yy.max()),\n",
    "       title='Neural Network Classification Landscape')\n",
    "ax.grid(linestyle=':')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "The code below is slightly modified from the [official PyTorch tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) on image classifiers. \n",
    "\n",
    "If you are interested in building these systems on your own data, I highly recomment you start by working through the [60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) tutorial. Then, look at the [Examples Repository](https://github.com/pytorch/examples/) for some concrete examples that do various things, and finally, get very very familiar with the [PyTorch API Documentation](https://pytorch.org/docs/stable/) page. If you write PyTorch code as often as I do, you'll have that page almost permanently open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We start by importing modules. Don't worry too much about these right now; they basically give us the functionality we need to put together our model. As you learn more about PyTorch, you'll learn which packages are used more often (like `torch.nn` and `torch.nn.functional`) and which are more specialized, and also what the conventions are for importing different packages (like the fact that `torch.nn.functional` is often imported as `F`). These are optional, but highly recommended so people can quickly understand your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "Right away, we can design our network. In practice the network code would be hidden in an external script and then imported, but since this is a tutorial notebook it makes sense to put it here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "PyTorch model classes are typically written in a similar way; they must include at least two methods:\n",
    "\n",
    "- The `init()` method defines the \"building blocks\" of the network which you will use in the forward pass. It also allows you to pass stuff to the constructor -- for example, the number of filters you want, allowing you to change the architecture by passing in different arguments.\n",
    "- The `forward()` method defines how the building blocks are assembled into a full network. The input to the forward pass is what gets passed when you call `model(data)`, and the output values are what gets returned at the end.\n",
    "\n",
    "The `forward()` function is key; by defining your network here, PyTorch is able to calculate the gradient automatically through backpropagation. \n",
    "\n",
    "Below I've written out some concise guidelines for each of the elements used in the code. Clicking on the headers will take you to the PyTorch documentation for that function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d)\n",
    "\n",
    "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)`\n",
    "\n",
    "Applies a 2D convolution over an input signal composed of several input planes. The arguments used below are:\n",
    "\n",
    "- `in_channels`: How many channels are coming into this layer?\n",
    "- `out_channels`: How many channels are being sent out of this layer? (Think of this like the number of filters you want to use to represent the data at the current scale)\n",
    "- `kernel_size`: How big of a kernel do you want to use to represent your data? Think of this like the \"neighborhood\" of your convolution.\n",
    "\n",
    "The other arguments (`stride`, `padding`, `dilation`, `groups`, and `bias`) all maintain their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [nn.MaxPool2d](https://pytorch.org/docs/stable/nn.html#maxpool2d)\n",
    "\n",
    "`torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n",
    "\n",
    "Downsamples the input signal through maximum pooling. This is a straightforward operation which does NOT have weights / biases trained on it. Below, the arguments are:\n",
    "\n",
    "- `kernel_size`: The size of the window to use for performing the downsampling.\n",
    "- `stride`: How much to move the filter at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [nn.Linear](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "\n",
    "This is equivalent to a \"traditional\" (fully-connected) neural network layer:\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{x}\\mathbf{W}^{T} + \\mathbf{b}$$\n",
    "\n",
    "All you really need for this layer are:\n",
    "\n",
    "- `in_features`: The number of input neurons coming into this layer.\n",
    "- `out_features`: The number of outputs from this layer -- equivalent to the number of \"hidden\" neurons at the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [nn.functional.relu](https://pytorch.org/docs/stable/nn.html#id26)\n",
    "\n",
    "`torch.nn.functional.relu(input, inplace=False)`\n",
    "\n",
    "Applies the rectified linear unit (ReLU) nonlinearity to the input data:\n",
    "\n",
    "$$ \\textrm{ReLU}(x) = \\max{(0, x)} $$\n",
    "\n",
    "This is the \"functional\" version of `nn.relu()`; this means that you don't need to define it in the `init()` method before applying it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Data Through the Network\n",
    "\n",
    "You need to do a bit of math to figure out the size of the data as it moves through the network. If you don't do this properly, you'll get errors when you try to create the model. \n",
    "\n",
    "The network below has the following architecture:\n",
    "\n",
    "- conv1: Conv2d\n",
    "    - Output channels: 6, Kernel size: 5, Stride: 1, Padding: 0, Dilation: 1\n",
    "        - Change: **32x32x3** -> **28x28x6**\n",
    "        - Explanation: A kernel size of 5 with no padding means that the filter will start 2 pixels \"in\" on each side, so the height and width is reduced by 4\n",
    "- ReLu: No change in data\n",
    "- pool: MaxPool2d\n",
    "    - Kernel size: 2, Stride: 2\n",
    "        - Change: **28x28x6** -> **14x14x6** \n",
    "        - Explanation: Data downsampled by 50%\n",
    "- conv2: Conv2D\n",
    "    - Output channels: 16, Kernel size: 5, Stride: 1, Padding: 0, Dilation: 1\n",
    "        - Change: **14x14x6** -> **10x10x16** \n",
    "        - Explanation: Same kernel size, padding, and stride as conv1, so again reduce height and width by 4\n",
    "- ReLu: No change in data\n",
    "- pool: MaxPool2d\n",
    "    - Same parameters as previous pooling layer\n",
    "        - Change: **10x10x16** -> **5x5x16** \n",
    "        - Explanation: Downsampled by 50%\n",
    "- fc1: Linear\n",
    "    - Input features: 16 * 5 * 5 elements (\"flattened\" image from previous layer)\n",
    "    - Output features: 120\n",
    "- ReLu: No change in data\n",
    "- fc2: Linear\n",
    "    - Input features: 120\n",
    "    - Output features: 84\n",
    "- ReLu: No change in data\n",
    "- fc3: Linear\n",
    "    - Input features: 84\n",
    "    - Output features: 10 (number of desired classes -- these are our output nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        # Notes: \n",
    "        #  - The number of input channels to conv1 (the first convolutional layer) is the number of channels\n",
    "        #    in our input data. For RGB images, this should be 3.\n",
    "        #  - The number of output channels of conv1 is 6; since this data will then be passed\n",
    "        #    to conv2, then the number of input channels to conv2 should also be 6.\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        \n",
    "        # Define the pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Define the fully-connected layers\n",
    "        # Notes:\n",
    "        #  - You need to figure out how many input values you have to the first fully-convolutional layer\n",
    "        #    which means you have to do the work in the previous section to figure out what size your image\n",
    "        #    is going to be at this point in the network.\n",
    "        #  - The number of output nodes here is up to you; whether this expands and then contracts, or just \n",
    "        #    contracts, is a matter of preference.\n",
    "        #  - The final linear layer should have a number of outputs equivalent to the number of classes you want.\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Know Your Model (Visualizing and Understanding Model Parameters)\n",
    "\n",
    "You can visualize the model structure easily by calling methods that work on `nn.Module` classes. See [the documentation of nn.Module](https://pytorch.org/docs/stable/nn.html#module) for a list of the valid methods you can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Name:\\t\\tParameters Size:')\n",
    "print('-----\\t\\t----------------')\n",
    "for name, param in net.named_parameters():\n",
    "    print('{}\\t{}'.format(name, np.array(param.size())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the number of parameters listed above by adding together the parameters in each collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*3*5*5 + 6 + 16*6*5*5 + 16 + 120*400 + 120 + 84*120 + 84 + 10*84 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this in a bit more of a pythonic way, rather than simply typing in all the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of trainable parameters: {}'.format(sum(p.numel() for p in net.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requires_grad` property says whether the associated parameter is \"trainable\" -- if this was false, then the optimizer won't update those weights during training. We'll talk about this a bit more later when we actually train the model.\n",
    "\n",
    "If you want, you could even go crazy and look at the exact values of each of the parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in [p for p in net.parameters() if p.requires_grad]:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Data: Datasets, Dataloaders\n",
    "\n",
    "For this demo we're using the [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This is such a common dataset for demonstrating and testing CNNs that the `torchvision` package has special methods for automatically downloading and working with this dataset.\n",
    "\n",
    "The first thing we want to do is create a PyTorch \"transform\" for our images -- this will convert the input images to the proper format (PyTorch Tensor), and will normalize the images as well. We will use this transform in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define \"datasets\" and \"dataloaders\". These are PyTorch-specific things which take care of data input-output, as well as creating batches, randomizing the order during training, etc. They are basically iterators that allow you to \"step through\" a dataset -- we can iterate through them to pull out samples, either for training or testing.\n",
    "\n",
    "If you have different types of datasets, you can load them in various ways. Check out:\n",
    "\n",
    "- The [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) documentation on the different classes and functions for defining datasets;\n",
    "- The [`torchvision.datasets`](https://pytorch.org/docs/stable/torchvision/datasets.html) page for a list of pre-defined benchmark image databases;\n",
    "- The [`torchvision.datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) section for building your own image database starting with image files contained in class folders;\n",
    "- The [`torchvision.datasets.DatasetFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder) for a format-agnostic version of `ImageFolder`; and finally\n",
    "- The [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for a nice example of how to build your own dataset code, allowing you to do some neat things with multi-modal datasets (images plus landmarks plus captions?)\n",
    "\n",
    "Note that since we have different requirements for training and testing, we have to create two different dataloaders (for example, the test loader doesn't have to shuffle the data, might require different transforms, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these are image-based datasets, we want to create a small helper function for displaying an image (meaning we don't have to type the same bit of code over and over):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # Un-normalize the image\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    # Need to transpose the image to get the channels in the right spot\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the dataloaders, you first create an iterator that you can then use to \"click\" through a dataset. Each time you call the `.next()` method on the iterator, it will generate a new batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for the training dataset\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "# Call the `.next()` method to get a batch of images and list of corresponding class labels\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# It's a good idea to make sure we actually got some data that we can use, and what size / format it's in\n",
    "print('Size of the image batch: {}'.format(images.size()))\n",
    "print('Size of the labels batch: {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the size of the image batch: the way that PyTorch organizes image batches, the dimensions are: `[batches channels height width]`. \n",
    "\n",
    "Often, we want to view a batch of samples at once; because this is such a common task, there is a utility function `torchvision.utils.make_grid()` which takes in a batch of images and returns a stitched grid of images consisting of all the images in a batch. \n",
    "\n",
    "(The channels aren't fixed though, so we still have to do the \"transpose\" inside our helper `imshow` function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(images, pad_value=1))\n",
    "\n",
    "# Print the labels as well\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have some images and a way of accessing them, as well as a way to figure out the class of the image. Now we can actually begin training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Loss Functions, Optimizer, and Backpropagation\n",
    "\n",
    "Now we can move on to training! But first, there are two more important things we need to identify:\n",
    "\n",
    "- The ***Loss Function***, which is responsible for penalizing our network for mistakes AND for computing the gradient for all the weights in the network;\n",
    "- The ***Optimization Algorithm***, which actually goes about changing the weights in response to the gradient calculated by the loss function.\n",
    "\n",
    "These are separate algorithms and are defined separately in PyTorch.\n",
    "\n",
    "### Selecting a Loss Function\n",
    "\n",
    "We can either define our own function or we can use one provided by PyTorch. Look [here in the documentation](https://pytorch.org/docs/stable/nn.html#loss-functions) for a list of possible loss functions and what they're used for. \n",
    "\n",
    "The main thing for the loss functions is that they all contain a `.backward()` method, which allows you to automatically calculate the gradient for the network parameters. Actually **how** this works is beyond the scope of this network, but you can start by looking at the [documentation for PyTorch's Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html). For now, it's fine to just consider it \"magic.\"\n",
    "\n",
    "For our purposes, we'll be using [the Cross Entropy Loss function](https://pytorch.org/docs/stable/nn.html#crossentropyloss). \n",
    "\n",
    "#### Pay Attention to your Loss Functions!\n",
    "\n",
    "It is **critical** to understand what values your loss function expects and what it's comparing them to. Study the loss function API and associated examples carefully, so you know what shape your inputs and labels should be and what their values should contain. \n",
    "\n",
    "`CrossEntropyLoss` combines a [log softmax function, `LogSoftmax`](https://pytorch.org/docs/stable/nn.html#logsoftmax), which takes the output of the network and makes the values sum to 1, and a [negative log likelihood loss function, `NLLLoss`](https://pytorch.org/docs/stable/nn.html#nllloss). This is why we don't need to have a `Softmax` in the network definition (because our loss function takes care of that). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting an Optimizer\n",
    "\n",
    "Here, we [select an optimizer](https://pytorch.org/docs/stable/optim.html) -- this is the function responsible for updating our parameter values.\n",
    "\n",
    "There are several algorithms implemented in `torch.optim`, but they all require two things: \n",
    "\n",
    "- A set of parameters that are going to be changed -- in our case, this is the list of the trainable parameters of the network, which we get by `net.parameters()`; and\n",
    "- A set of \"hyper-parameters\" that control the behavior of the optimizer -- this is things like learning rate `lr` and momentum.\n",
    "\n",
    "You can also specify different hyper-parameters for different parts of the network, for example if you wanted the convolutional layers of the network to learn faster than the fully-connected layers. We won't do this, but sometimes you may have a good reason for tweaking with these settings.\n",
    "\n",
    "#### Which Optimizer to Use?\n",
    "\n",
    "Just like selecting nonlinearities, which optimizer to use is a matter of debate and subjectivity. The goal of the optimizer is to train the network smoothly, without over-fitting -- if you have a system that either does not learn anything, or which over-fits very quickly, it may be worth it to switch optimizers or the hyper-parameters.\n",
    "\n",
    "In our case, we will use the [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure\n",
    "\n",
    "Now, we basically have to run our training algorithm in one big loop. At each epoch, the process iterates through a \"mini-batch\" of samples. The main process for each batch is:\n",
    "\n",
    "1. Get the inputs (images) and the labels for each sample in the batch\n",
    "2. Zero out the parameter gradients (so we calculate them anew for each batch)\n",
    "3. Run the forward pass through the network to get the current outputs\n",
    "4. Calculate the loss of the network in the batch by comparing the outputs to the labels\n",
    "5. Calculate the gradient of the network by calling the loss function's `.backward()` method\n",
    "6. Use the optimizer to calculate the next step of weight modifications, and apply them\n",
    "7. Calculate the loss for this batch, and add it to a running average of loss values\n",
    "\n",
    "Each of these steps is highlighted in the code below.\n",
    "\n",
    "We repeat this for each batch of samples in the dataloader; once we finish one pass through the dataloader, we have completed one epoch. We can then repeat epochs over and over until we reach some stopping point. For now, we will just run two epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "for epoch in range(2):\n",
    "\n",
    "    # Reset the loss for this epoch\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate through the training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Pull out this batch of data \n",
    "        # Recall that the trainloader was created with the option `batch_size=4`\n",
    "        # Thus, each time we get `inputs` and `labels`, we get the data for 4 samples\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        # This is necessary to prevent \"accumulation\" of the gradients over multiple training batches\n",
    "        # Each batch calculates the gradient independently\n",
    "        # This is critical for allowing the system to learn in parallel\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the forward pass for these input samples\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # Calculate the loss by comparing the network output to the ground-truth labels (from trainloader)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate the backward pass (backpropagation) for the loss function\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calculate the optimization step, tweaking the actual parameters (weights) of the network\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the loss of this batch to the running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print the loss every 2000 batches and then reset it\n",
    "        if i % 2000 == 1999:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model on Testing Data\n",
    "\n",
    "Let's see how well our trained model can predict the labels of testing data -- which again, remember, we have not yet seen.\n",
    "\n",
    "### Viewing a Single Batch\n",
    "\n",
    "First, let's pull out a single batch of testing data and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Display the images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# Print out the ground truth labels for the testing data\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put the data through the network -- this is a simple forward pass, and should be quick.\n",
    "\n",
    "We also print the outputs as well; each column of the output is a sample, and each row is one of the class outputs. \n",
    "\n",
    "**Remember:** The magnitude of the network output is proportional to the probability that the sample belongs to that class, but since we haven't used a `softmax()` layer in our network, these outputs are not probabilities (the do not sum to 1, they can be greater than 1, and they can be less than 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "\n",
    "print('Raw network output:')\n",
    "print(torch.transpose(outputs, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a class label by finding the row (class) of the maximum value for each sample, and then using the class index to print out the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Ground Truth:\\t', '\\t'.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "print('Predict:\\t', '\\t'.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating All Testing Data\n",
    "\n",
    "Let's iterate through the testing dataloader, pulling out our samples and performing the same operations as we just did on each batch. We'll keep track of the total number of samples as well as the correct samples to calculate our accuracy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# This line tells PyTorch to not keep track of gradients for our testing forward passes\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we have 10 classes, so if our classifier is randomly guessing, we would expect an accuracy of 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Class-by-Class Performance\n",
    "\n",
    "We might be interested in seeing how the performance varies based on the target class -- maybe it's harder to distinguish planes than cats?\n",
    "\n",
    "So here we're just going to do the same thing as we just did, except our `total` and `correct` counts will be a list where each list element holds the data for a different class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Visualizing Filter Outputs\n",
    "\n",
    "Fundamentally the convolutional layers are calculating filters of the input images. We can pull these out of our trained network to see what they look like, which may give us an idea of what characteristics of the image the system is looking at.\n",
    "\n",
    "In PyTorch, we can create an \"intermediate\" CNN by slicing into the network's children and pulling out only the first 2 or 3 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the intermediate layers to see what the data looks like as it travels through the trained network\n",
    "net_intermediate = nn.Sequential(*list(net.children())[0:1])\n",
    "net_intermediate.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This intermediate network's output is now just the output of one of the convolutional layers. We can push some testing data through the network and visualize the output the same way we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_outputs = net_intermediate(images)\n",
    "\n",
    "# Calculate the size of the outputs to construct our matplotlib plot\n",
    "nImages, nFilters, height, width = image_outputs.shape\n",
    "\n",
    "f, ax = plt.subplots(nImages, nFilters+1, figsize=(25,12))\n",
    "\n",
    "for idx_img in range(nImages):\n",
    "    ax[idx_img][0].imshow(np.moveaxis(images[idx_img,:,:,:].numpy(), 0, -1))\n",
    "    for idx_filter in range(nFilters):\n",
    "        ax[idx_img][idx_filter+1].imshow(image_outputs[idx_img,idx_filter,:,:].detach().numpy(), cmap=plt.cm.gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
